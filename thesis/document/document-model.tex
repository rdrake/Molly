% !TEX root = Thesis.tex
\section{Document Model}
\label{sec:document-model}
	In contrast to the relational model, the document model represents semi-structured as well as unstructured data.  Examples of information suitable to the document model includes emails, memos, book chapters, etc.
	
	These pieces, or units, of information are broken into documents.  Groups of related documents (for example, a library catalogue) are referred to as a document collection.

	\begin{defn}[Terms and Document]
	\label{def:document}
		A term, \(\term\), is an indivisible string (e.g.~a proper noun, word, or a phrase).	A document, \(\doc\), is a bag of words; order is irrelevant.
		
		Let \(\freq(\term, \doc)\) be the frequency of term \(\term\) in \(\doc\), \(\terms\) denote all possible terms, and \(\bag{\terms}\) be all possible bag of terms.
	\end{defn}
	
	\begin{remark}
		We use the bag-of-words model for documents.  This means that position information of terms in a document is irrelevant, but the frequency of terms are kept in the document.  Documents are non-distinct sets.
	\end{remark}
	
	\begin{defn}[Document Collection]
	\label{def:document-collection}
		A document collection \(\dc\) is a set of documents, written \(\dc = \{\doc_1, \doc_2, \dotsc, \doc_k\}\).  The cardinality of \(\dc\) is denoted by \(\gls{ndocs}\).
	\end{defn}
	
	\begin{ex}
	\label{ex:documents}
		Consider the following short phrases
		
		\begin{enumerate}
			\item math 360 is a math class
			\item cdps 101 is a boring lecture
			\item mathematics lecture was great
		\end{enumerate}
		
		Each sentence phrase produces a document, giving us the following
		\begin{align}
			\doc_1 &= \{\text{``math''}: 2, \text{``a''}: 1, \text{``is''}: 1, \text{``360''}: 1, \text{``class''}: 1\} \\
			\doc_2 &= \{\text{``a''}: 1, \text{``boring''}: 1, \text{``is''}: 1, \text{``cdps''}: 1, \text{``lecture''}: 1, \text{``101''}: 1\} \\
			\doc_3 &= \{\text{``mathematics''}: 1, \text{``great''}: 1, \text{``was''}: 1, \text{``lecture''}: 1\}
		\end{align}
	\end{ex}
	
	\subsection{Vectorization of Documents}
	\label{sec:vectorization-of-documents}
		One of the most fundamental approaches for searching documents is to treat documents as high-dimensional vectors, and the document collection as a subset in a vector space.  Search queries become a nearest neighbour search in a vector space using a distance metric.
		
		The first step is to convert a bag of terms into vectors.  The standard technique \cite{ir-08} uses a scoring function that measures the relative importance of terms in documents.
		
		\begin{defn}[\Gls{tfidf} Score]
			The term frequency is the number of times a term \(\term\) appears in a document \(\doc\), as given by \(\freq(\term, \doc)\).	The document frequency of a term \(\term\), denoted by \(\df(\term)\), is the number of documents in \(\dc\) that contains \(\term\).	 It is defined as
			\[
				\df(\term) = \abs{\{\doc \in \dc: \term \in \doc\}}
			\]
			
			The combined \gls{tfidf} score of \(\term\) in a document \(\doc\) is given by
			\[
				\tfidf(\dc, \term, \doc) = \frac{\freq(\term, \doc)}{\abs{\doc}} \cdot \log{\frac{N}{\df(\term)}}
			\]
		\end{defn}
		
		The first component, \(\frac{\freq\left(\term, \doc\right)}{\abs{\doc}}\), measures the importance of a term within a document.  It is normalized to account for document length.	 The second component, \(\log{\frac{N}{\df\left(\term\right)}}\), is a measure of the rarity of the term within the document collection \(\dc\).
		
		\begin{ex}
			Using the documents from \vref{ex:documents}, the \gls{tfidf} scores are as follows.
			\[
				\bordermatrix{
					~ & d_1 & d_2 & d_3 \cr
					\term_{1} : \text{``101''} & 0.0000 & 0.2642 & 0.0000 \cr
					\term_{2} : \text{``360''} & 0.3170 & 0.0000 & 0.0000 \cr
					\term_{3} : \text{``a''} & 0.1170 & 0.0975 & 0.0000 \cr
					\term_{4} : \text{``boring''} & 0.0000 & 0.2642 & 0.0000 \cr
					\term_{5} : \text{``cdps''} & 0.0000 & 0.2642 & 0.0000 \cr
					\term_{6} : \text{``class''} & 0.3170 & 0.0000 & 0.0000 \cr
					\term_{7} : \text{``great''} & 0.0000 & 0.0000 & 0.3962 \cr
					\term_{8} : \text{``is''} & 0.1170 & 0.0975 & 0.0000 \cr
					\term_{9} : \text{``lecture''} & 0.0000 & 0.0975 & 0.1462 \cr
					\term_{10} : \text{``math''} & 0.6340 & 0.0000 & 0.0000 \cr
					\term_{11} : \text{``mathematics''} & 0.0000 & 0.0000 & 0.3962 \cr
					\term_{12} : \text{``was''} & 0.0000 & 0.0000 & 0.3962 \cr
				}
			\]
		\end{ex}

		\begin{defn}[Document Vector]
			Given a document collection \(\dc\) with \(M\) unique terms \(\terms = [\term_1, \term_2, \dotsc, \term_n ]\), each document \(\doc\) can be represented by an \(M\)-dimensional vector.
			\[
				\vec{\doc} = 
				\left[
				\begin{array}{c}
					\tfidf(\term_1, \doc) \\
					\tfidf(\term_2, \doc) \\
					\vdots \\
					\tfidf(\term_n, \doc)
				\end{array}
				\right]
			\]
		\end{defn}
		
		\begin{ex}
			The documents in \vref{ex:documents} would produce the following vectors.
			\[
				\vec{\doc_n} = 
					\left[
						\begin{array}{l}
							\tfidf(\term_{1}, \doc_{n}) \\
							\tfidf(\term_{2}, \doc_{n}) \\
							\tfidf(\term_{3}, \doc_{n}) \\
							\tfidf(\term_{4}, \doc_{n}) \\
							\tfidf(\term_{5}, \doc_{n}) \\
							\tfidf(\term_{6}, \doc_{n}) \\
							\tfidf(\term_{7}, \doc_{n}) \\
							\tfidf(\term_{8}, \doc_{n}) \\
							\tfidf(\term_{9}, \doc_{n}) \\
							\tfidf(\term_{10}, \doc_{n}) \\
							\tfidf(\term_{11}, \doc_{n}) \\
							\tfidf(\term_{12}, \doc_{n}) \\
						\end{array}
					\right],
				\vec{\doc_1} =
					\left[
						\begin{array}{c}
							0.0000 \\
							0.3170 \\
							0.1170 \\
							0.0000 \\
							0.0000 \\
							0.3170 \\
							0.0000 \\
							0.1170 \\
							0.0000 \\
							0.6340 \\
							0.0000 \\
							0.0000 \\
						\end{array}
					\right],
				\vec{\doc_2} =
					\left[
						\begin{array}{c}
							0.2642 \\
							0.0000 \\
							0.0975 \\
							0.2642 \\
							0.2642 \\
							0.0000 \\
							0.0000 \\
							0.0975 \\
							0.0975 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
						\end{array}
					\right],
				\vec{\doc_3} =
					\left[
						\begin{array}{c}
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.3962 \\
							0.0000 \\
							0.1462 \\
							0.0000 \\
							0.3962 \\
							0.3962 \\
						\end{array}
					\right]
			\]
		\end{ex}
		
		\begin{defn}[Search Query]
			A search query \(\q\) is simply a document (as defined by \vref{def:document}).  The top-\(k\) answers to \(\q\) with respect to a collection \(\dc\) is defined as the \(k\) documents, \(\{\doc_1, \doc_2, \dotsc, d_k\}\) in \(\dc\), such that \(\{\vec{\doc_1}, \vec{\doc_2}, \dotsc, \vec{d_k}\}\) are the closest vectors to \(\vec{\q}\) using a Euclidean distance measure in \(\mathbb{R}^N\).
		\end{defn}
		
		\begin{ex}
			Given the search query \(\q = \{\text{math}, \text{lecture}, \text{was}, \text{great}\}\), compute the vector \(\vec{\q}\) within the document collection \(\dc\) (as defined in \vref{ex:documents}).
			\[
				\vec{\q} = 
					\left[
						\begin{array}{c}
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.0000 \\
							0.2500 \\
							0.1038 \\
							0.1038 \\
							0.2500 \\
							0.0000 \\
							0.0000 \\
						\end{array}
					\right]
			\]
		\end{ex}
		
		In order to determine the top-\(k\) documents for search query \(\q\), we need a way of measuring the similarity between documents.
		
		\begin{defn}[Cosine Similarity]
			Given two document vectors, \(\vec{\doc}_1\) and \(\vec{\doc}_2\), the cosine similarity is the dot product \(\vec{\doc}_1 \cdot \vec{\doc}_2\), normalized by the product of the Euclidean distance of \(\vec{\doc}_1\) and \(\vec{\doc}_2\) in \(\mathbb{R}^N\).	It is denoted as \(\similarity(\vec{\doc}_1, \vec{\doc}_2)\).
			\begin{align}
				\similarity(\vec{\doc}_1, \vec{\doc}_2) &= \frac{\vec{\doc}_1 \cdot \vec{\doc}_2}{\norm{\vec{\doc}_1} \cdot \norm{\vec{\doc}_2}} \\
				 &= \frac{\sum\limits_{i=1}^{N} \vec{\doc}_{1, i} \times \vec{\doc}_{2, i}}{\sqrt{\sum\limits_{i=1}^{N} (\vec{\doc}_{1, i})^2} \times \sqrt{\sum\limits_{i=1}^{N} (\vec{\doc}_{2, i})^2}}
			\end{align}
		\end{defn}
		
		Recall we may represent search queries as documents and thus document vectors.	Therefore we may compute the score of a document \(\doc\) for a search query \(\q\) as
		\[
			\similarity(\vec{\doc}, \vec{\q})
		\]
		
		\begin{ex}
			Given the document collection \(\dc\) (from \vref{ex:documents}) and search query \(q\), compute the similarity between \(\q\) and every document \(\doc \in \dc\).
			\begin{align}
				\similarity(\vec{\doc_1}, \vec{\q}) = 0.390890 \\
				\similarity(\vec{\doc_2}, \vec{\q}) = 0.061592 \\
				\similarity(\vec{\doc_3}, \vec{\q}) = 0.252789
			\end{align}
		\end{ex}
		
	\subsection{Extending the Document Model}
	\label{sec:extending-the-document-model}
		In the extended document model, documents have fields, denoted as \(\fields{\doc}\), and each field has a value.  Thus
		\[
			\doc : \fields{\doc} \to \bag{\terms}
		\]
		
		\begin{ex}[Semi-Structured Document]
			We see that \(\doc_1\) is about MATH 360.  The document contents are semi-structured, containing both a course code and the subject ID.  By adding fields to the document, we are left with \vref{tbl:course-document}.
			
			\begin{table}
				\centering
				
				\begin{tabular}{ll}
					\toprule
					Attribute & Value \\
					\midrule
					code & CDPS 101 \\
					title & Human-Mutant Relations \\
					subject & Community Development \& Policy Studies \\
					\bottomrule
				\end{tabular}
				
				\caption{Properties of the Course titled Human-Mutant Relations.}
				\label{tbl:hmr-properties}
			\end{table}
			
			\begin{table}
				\centering
				
				\begin{tabular}{ll}
					\toprule
					Field & Value \\
					\midrule
					code & MATH 360 \\
					subject & MATH \\
					body & math 360 is a math class \\
					\bottomrule
				\end{tabular}
				
				\caption{Course document for MATH 360.}
				\label{tbl:course-document}
			\end{table}
			
			which is similar in structure to \vref{tbl:hmr-properties}.
		\end{ex}
		
	\subsection{Approximate String matching}
	\label{sec:n-gram}
		\begin{defn}[N-Gram]
			An \(n\)-gram is a contiguous sequence of substrings of string \(S\) of length \(n\).	 An algorithm for computing the \(n\)-gram of \(S\) is given in \vref{alg:n-gram}. 
		\end{defn}
		
		% \char"24 - DOLLAR	 BILL Y'ALL

		\begin{algorithm}[!ht]
			\caption{\(\textsc{N-Gram}(S, n, s)\)}
			\label{alg:n-gram}
			
			\begin{singlespaced}
				\begin{algorithmic}[1]
					\REQUIRE \(S\) is a string, \(n \ge 1\), and \(s\) is a character
					\ENSURE the list of \(n\)-grams of \(S\)
					\medskip
					\STATE \(G \leftarrow []\)\label{alg:n-gram:G}
					\STATE \(p \leftarrow \textsc{Repeat}(s, n - 1)\)
					\STATE \(S \leftarrow \textsc{Pad}(S, p)\)
					\STATE \(S \leftarrow \textsc{Replace}(S, `\;', p)\)\label{alg:n-gram:S}
					
					\FOR{\(i=0\) \TO \(l - n + 1\)}
						\STATE append \(S[i, i + n]\) to \(G\)
					\ENDFOR
					
					\RETURN \(G\)
					\medskip
					\medskip
				\end{algorithmic}
			\end{singlespaced}
		\end{algorithm}
		
		%\todo{The medskips above should not be required, but are in single spaced mode.}
		
		Where \(l\) is the length of \(S\), \(\textsc{Repeat}(S, n)\) repeats \(s\) character \(n\) times, \(\textsc{Pad}(S, p)\) prefixes and postfixes \(S\) with \(p\), and \(\textsc{Replace}(S, s, p)\) replaces character \(s\) with \(p\) in string \(S\).
		
		\begin{ex}
		\label{ex:ngram}
			Given a string \(S = \text{``human''}\), compute the trigram of \(S\) using \vref{alg:n-gram}.
			\[
				G = \{
					\text{``\char"24\char"24h''},
					\text{``\char"24hu''},
					\text{``hum''},
					\text{``uma''},
					\text{``man''},
					\text{``an\char"24''},
					\text{``n\char"24\char"24''}
				\}
			\]
		\end{ex}
		
		We use \(n\)-grams in order to permit approximate string matching.
		
		\begin{ex}
		\label{ex:n-gram-comparison}
			Given a string \(S\) (\vref{ex:ngram}), let \(S' = \text{``humans''}\).  Compute the trigram of \(S'\) and compare it to \(S\).
			\[
				G' = \{
					\text{``\char"24\char"24h''},
					\text{``\char"24hu''},
					\text{``hum''},
					\text{``uma''},
					\text{``man''},
					\text{``ans''},
					\text{``ns\char"24''},
					\text{``s\char"24\char"24''}
				\}
			\]
			
			Comparing \(G\) to \(G'\) results in the following matrix
			
			\begin{figure}
				\[
					\bordermatrix{
						~ & G & G' \cr
						\term_{1} : \text{``ns\char"24''} & 0 & 1 \cr
						\term_{2} : \text{``n\char"24\char"24''} & 1 & 0 \cr
						\term_{3} : \text{``s\char"24\char"24''} & 0 & 1 \cr
						\term_{4} : \text{``ans''} & 0 & 1 \cr
						\term_{5} : \text{``man''} & 1 & 1 \cr
						\term_{6} : \text{``uma''} & 1 & 1 \cr
						\term_{7} : \text{``\char"24\char"24h''} & 1 & 1 \cr
						\term_{8} : \text{``hum''} & 1 & 1 \cr
						\term_{9} : \text{``\char"24hu''} & 1 & 1 \cr
						\term_{10} : \text{``an\char"24''} & 1 & 0 \cr
					}
				\]
				
				\caption{Comparison between \(n\)-grams of \(G\) and \(G'\).}
				\label{fig:n-gram-misspelling-comparison}
			\end{figure}
			
			As \vref{fig:n-gram-misspelling-comparison} shows, using \(n\)-grams yield a similarity of \(\frac{5}{10}\).
		\end{ex}
			
	\subsection{Pros and Cons of the Document Model}
		There are numerous reasons to use the document model.  The most significant reason is that it allows users without domain knowledge and working knowledge of a complex query language such as \gls{sql} to find information.
		
		\begin{ex}[Simple Queries]
			Find all documents related to ``mathematics'' or ``lecture''.  The result of the query \(\q\) would be
			\[
				\query(\text{``mathematics''}) \cup \query(\text{``lecture''}) \rightarrow \{\doc_2, \doc_3\}
			\]
		\end{ex}
		
		Users can also modify queries to require certain terms be present or not present.
		
		\begin{ex}[\texttt{AND} Query]
		\label{ex:and-query}
			Find all documents containing both ``mathematics'' and ``lecture''.  This query would return the following set of documents
			\[
				\query(\text{``mathematics''}) \cap \query(\text{``lecture''}) \rightarrow \{\doc_3\}
			\]
			
			as only \(\doc_1\) contains both terms.
		\end{ex}
		
		\begin{ex}[\texttt{NOT} Query]
			Find all documents containing ``mathematics'' but not ``lecture''.  This query would return different results than \vref{ex:and-query}.
			\[
				\query(\text{``mathematics''})\neg\query(\text{``lecture''}) \rightarrow \emptyset
			\]
		\end{ex}
		
		While none of the above queries required domain knowledge, it is possible to use the extended document model (\vref{sec:extending-the-document-model}) to search specific fields.  Doing so permits users to leverage their existing domain knowledge in order to achieve finer control over what documents are retrieved.
		
		\begin{ex}[Extended Query]
			Find all documents with a subject of ``MATH'' that contain the term ``class''.
			\[
				\query(\text{``subject''}, \text{``MATH''}) \cap \query(\text{``class''}) \rightarrow \{\doc_1\}
			\]
		\end{ex}
		
		Not only does the document model provide a familiar interface to search for information with, it also ranks the results.  In the relational model a search for ``mathematics'' would return all named tuples that contained that term.  In the document model, documents are ranked against the query \(\q\) and the top-\(k\) documents are returned.
		
		The advantage is that users have the result of \(\q\) already ranked so only the most relevant documents may be explored.	 As the number of documents matching \(\q\) for a large corpus can be high, showing only the top-\(k\) relevant documents may save the user a substantial amount of time.
		
		The relational model does not permit approximate string matching.  By utilizing the document model with \(n\)-grams (\vref{sec:n-gram}), users who substitute, delete, or insert characters from the desired term may still receive results for their intended term (see \vref{ex:n-gram-comparison} for a demonstration of how \(n\)-grams overcome character insertion).
		
		Unfortunately the document model does not support the concept of foreign keys (\vref{def:foreign-keys}).  While information is easily accessible due to flexible search, each document is a discrete unit of information.  Aggregate queries are unsupported, as these units are not linked amongst one another.