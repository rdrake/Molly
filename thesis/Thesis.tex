% Change "draft" to "final" when you're done.
\documentclass[12pt,letterpaper,oneside,draft]{report}

\usepackage[top=1in, left=1.5in, right=1in, bottom=1in]{geometry}

\usepackage{fontspec,lipsum}
\usepackage{mathpazo}
\defaultfontfeatures{Ligatures=TeX}
\usepackage[small,sf,bf]{titlesec}

\usepackage{xltxtra}
\usepackage{xunicode}

%\DisemulatePackage{setspace}
\usepackage{setspace}
\onehalfspacing

\usepackage{titling}

\usepackage{unicode-math}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algorithmic}

%\usepackage{minted}
\usepackage{tikz}

\usepackage{nag}

\setmainfont[
	Numbers=OldStyle,
	Kerning=Uppercase,
	SizeFeatures={
		{Size={8-10}, Font=* Caption},
		{Size={10.01-}, Font=*}
	}]{Garamond Premier Pro}
\setsansfont{Futura Std}
\setmathfont{STIXGeneral}
\setmonofont{Consolas}

\pagenumbering{roman}

\newcommand{\thesistitle}[8]{
	\title{#1}
	\author{#2}

	\begin{titlepage}
		\centering
		\vspace{2cm}
			{\huge\sc #1}\\[1cm]

			by\\[1cm]

			{\Large #2}\\[2cm]

			\singlespacing{A Thesis Submitted in Partial Fulfillment\\of the Requirements for the Degree of}\\[1cm]

			#3\\[0.5cm]

			in\\[0.5cm]

			Faculty of #4\\[0.5cm]

			#5\\[1.5cm]

			University of Ontario Institute of Technology\\[1.5cm]

			Supervisor:  #6\\[1.5cm]

			#7 #8\\[3cm]

			Copyright \copyright\ #2 #8
	\end{titlepage}}

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\begin{document}
	\thesistitle{Molly}{Richard Drake}{Masters of Science}{Science}{Computer Science}{Dr. Ken Q. Pu}{September}{2012}

	\begin{abstract}
		\lipsum[2]
	\end{abstract}

%	\begin{acknowledgements}
%		\lipsum[2]
%	\end{acknowledgements}

	\chapter*{Preface}
		\section*{Background and Motivation}
			The introduction of keyword search has revolutionized how we find information.  Over the years, numerous techniques have been developed which make searching through large amounts of information for one or more keywords extremely fast.  With the advent of faster computer hardware, we are able to not only search through small attributes of a document (eg. Title, synopsis, etc.) but rather the entirety of the document itself.
		
			An example of an early system which utilized keyword search would be a library catalogue.  Such a system would allow a user to search, by keyword, for the title of a book, manuscript, etc.  The results would show item titles matching the keyword(s), as well as other information such as whether or not the item is in circulation, as well as where it is located within the library.  This information would come from a relational database.
		
			With the rise of the World Wide Web, much information was placed online.  This information would be easy to access if one knew how to locate it.  Unfortunately, over time, so much information existed on the World Wide Web that it became difficult to keep track of it all.  There was a need to index all of this information and make it accessible.  This need was filled by a Web search engine.
		
			Initial Web search engines comprised of simple scripts that gathered listings of files on FTP servers; they were essentially link farms.  A few short years later, the first full-text (keyword) search engine, WebCrawler, was released.
		
			While full-text search engines provided an excellent means for locating information, as the Web grew larger, the volume of noise also grew larger.  In addition, every Web page could be structured in a different way; the Web was largely a collection of unstructured documents.  That is, there were few obvious links between them.
		
			Search engines such as Google attempted to solve this problem by introducing new algorithms, such as PageRank, to rank Web pages on both the relevance of their content as well as their reputation.  The idea was if a page is linked to often, it is considered to be more authoritative on a subject than a page with fewer links.  This allowed the relevance of a page to be computed based on not only its contents, but its artificial importance.
		
			Molly attempts to avoid some of the issues plaguing search engines.  It deals primarily with structured, filtered data.  This allows us to provide results with less noise.  In addition, the fact that it deals with structured data means links between documents are explicitly stated.  Rather than inferring a link between documents based on hyperlinks, we know when two documents are linked together.
		
			This thesis provides an overview of the Molly system.
		
		\section*{Outline}
			Chapter 1 provides a look at systems which tried to solve a similar problem.  It analyzes where other systems have failed and where they have succeeded.
			
			Chapter 2 discusses the system design and rationale behind the design.
			
			Chapter 3 details the implementation of Molly.  It also discusses several factors which influence performance.
			
			Chapter 4 moves beyond single-threaded performance and discusses making the system use multiple threads simultaneously.  It also discusses what performance gains, if any, were made.

	\tableofcontents

	\listoftables
	\listoffigures
	\listofalgorithms

	\clearpage
	\pagenumbering{arabic}
	
	\setcounter{chapter}{-1}
	\chapter{Data Representation and Notation}

	\chapter{Literature Review}

	\chapter{System Design, Rationale, \& Performance}
		\section{Introduction}
			\subsection{Data Representation}
				The data from the database is represented in various data structures.  There are separate representations for each type of data:  values, entities, and entity groups.

				\subsubsection{Value}
					\begin{defn}
						A \textbf{Value} represents a single piece of information.  To avoid repetition, each value is unique.  That is, $\exists! v \in V$, where $v$ is a value in the set $V$ of all values.
					\end{defn}

				\subsubsection{Entity}
					\begin{defn}
						An \textbf{Entity} is a collection of attributes, $a_n$, each mapped to a single value, $v_n$.  An entity also includes additional information such as a unique identifier.

						\begin{figure}[!ht]
							\centering
							\[
								\begin{array}{ll}
									\mathrm{id} & T_n|v_{id} \\
									a_1 & v_1 \\
									a_2 & v_2 \\
									\vdots & \vdots \\
									a_n & v_n
								\end{array}
							\]
							\caption{The structure of an entity}
							\label{fig:entity-rep}
						\end{figure}

						Entities are analogous to rows in a database table.  Thus, the unique identifier is generated based on the table name, $T_n$, as well as unique key in the table, $v_{id}$.  The unique key identifies the row, and the table name identifies the table.  Together they uniquely identify the entity within the entire database.

						$\exists! e_{id} \in E$, where $E$ is the set of all entities.
					\end{defn}

				\subsubsection{Entity Group}
					\begin{defn}
						An \textbf{Entity Group} joins together two or more entities.  These entity groups can also have attributes, $a_n$, and values, $v_n$, associated with them much like entities.

						\begin{figure}[!ht]
							\centering
							\[
								\begin{array}{ll}
									e_L & \left[ e_1, e_2, \ldots, e_n \right] \\
									a_1 & v_1 \\
									a_2 & v_2 \\
									\vdots & \vdots \\
									a_n & v_n
								\end{array}
							\]
							\caption{The structure of an entity group}
							\label{fig:entity-group-rep}
						\end{figure}

						\begin{tikzpicture}
							[scale=.8,auto=left,every node/.style={circle}]
							\node (e1) at (1,1) {$e_1$};
							\node (e2) at (4,2)  {$e_2$};
							\node (e3) at (7,3)  {$e_3$};
							\node (e4) at (10,3) {$e_4$};
							\node (e5) at (13,2)  {$e_5$};
							\node (e6) at (16,1)  {$e_6$};

							\foreach \from/\to in {e1/e2,e2/e3,e3/e4,e4/e5,e5/e6}
								\draw (\from) -- (\to);

						\end{tikzpicture}
					\end{defn}

			\section{Overview of System Components}
				\subsection{Crawler}
				\subsection{Indexer}
				\subsection{Core (Processing)}
				\subsection{API}
				\subsection{Frontend}

			\section{Data Flow}
			
			\section{Implementation Issues}

		\section{Ford-Fulkerson}
			\begin{algorithm}
				\begin{algorithmic}
					\ENSURE $1=1$
				\end{algorithmic}
			\end{algorithm}

	
	\chapter{Blah blah blah}
		\section{Choosing a Breadth-First Search Algorithm}
			We took several criteria into consideration when choosing a Breadth-First Search algorithm to perform the graph search.  In the context of this problem, there is no obvious heuristic to help predict the distance between two nodes.  We decided to use a constant cost function.  That is, $d_{(v_1, v_2)} = n$.  The selected algorithm must also be highly parallelizable.

			

			BFS - Wrong result if non-uniform distance
			Bellman-Ford - Always correct result
			Dijkstra - Faster than Bellman-Ford, uses priority queue
			Push-Relabel \& Johnson - Uses complicated (and difficult to parallelize) data structures
			A-Star Search - No obvious heuristic
	
	
\end{document}
